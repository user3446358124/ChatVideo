{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import Config\n",
    "config_file = \"configs/config_phi.json\"\n",
    "cfg = Config.from_file(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "from models import VideoChat2_it_phi\n",
    "from utils.easydict import EasyDict\n",
    "import torch\n",
    "\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from decord import VideoReader, cpu\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import PILToTensor\n",
    "from torchvision import transforms\n",
    "from dataset.video_transforms import (\n",
    "    GroupNormalize, GroupScale, GroupCenterCrop, \n",
    "    Stack, ToTorchFormatTensor\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Video, HTML\n",
    "\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import copy\n",
    "\n",
    "import json\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import decord\n",
    "decord.bridge.set_bridge(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74fa48f6a9b47bf9b996ddaf87094c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load stage2 model\n",
    "cfg.model.vision_encoder.num_frames = 4\n",
    "model = VideoChat2_it_phi(config=cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add lora to run stage3 model\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, \n",
    "    r=16, lora_alpha=32, lora_dropout=0.,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "         \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"\n",
    "    ]\n",
    ")\n",
    "model.phi_model = get_peft_model(model.phi_model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['phi_model.base_model.model.model.embed_tokens.weight', 'phi_model.base_model.model.model.layers.0.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.0.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.0.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.0.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.0.input_layernorm.weight', 'phi_model.base_model.model.model.layers.0.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.1.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.1.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.1.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.1.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.1.input_layernorm.weight', 'phi_model.base_model.model.model.layers.1.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.2.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.2.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.2.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.2.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.2.input_layernorm.weight', 'phi_model.base_model.model.model.layers.2.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.3.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.3.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.3.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.3.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.3.input_layernorm.weight', 'phi_model.base_model.model.model.layers.3.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.4.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.4.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.4.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.4.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.4.input_layernorm.weight', 'phi_model.base_model.model.model.layers.4.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.5.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.5.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.5.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.5.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.5.input_layernorm.weight', 'phi_model.base_model.model.model.layers.5.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.6.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.6.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.6.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.6.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.6.input_layernorm.weight', 'phi_model.base_model.model.model.layers.6.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.7.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.7.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.7.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.7.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.7.input_layernorm.weight', 'phi_model.base_model.model.model.layers.7.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.8.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.8.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.8.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.8.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.8.input_layernorm.weight', 'phi_model.base_model.model.model.layers.8.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.9.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.9.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.9.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.9.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.9.input_layernorm.weight', 'phi_model.base_model.model.model.layers.9.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.10.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.10.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.10.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.10.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.10.input_layernorm.weight', 'phi_model.base_model.model.model.layers.10.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.11.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.11.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.11.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.11.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.11.input_layernorm.weight', 'phi_model.base_model.model.model.layers.11.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.12.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.12.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.12.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.12.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.12.input_layernorm.weight', 'phi_model.base_model.model.model.layers.12.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.13.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.13.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.13.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.13.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.13.input_layernorm.weight', 'phi_model.base_model.model.model.layers.13.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.14.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.14.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.14.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.14.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.14.input_layernorm.weight', 'phi_model.base_model.model.model.layers.14.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.15.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.15.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.15.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.15.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.15.input_layernorm.weight', 'phi_model.base_model.model.model.layers.15.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.16.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.16.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.16.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.16.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.16.input_layernorm.weight', 'phi_model.base_model.model.model.layers.16.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.17.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.17.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.17.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.17.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.17.input_layernorm.weight', 'phi_model.base_model.model.model.layers.17.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.18.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.18.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.18.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.18.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.18.input_layernorm.weight', 'phi_model.base_model.model.model.layers.18.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.19.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.19.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.19.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.19.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.19.input_layernorm.weight', 'phi_model.base_model.model.model.layers.19.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.20.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.20.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.20.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.20.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.20.input_layernorm.weight', 'phi_model.base_model.model.model.layers.20.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.21.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.21.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.21.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.21.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.21.input_layernorm.weight', 'phi_model.base_model.model.model.layers.21.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.22.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.22.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.22.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.22.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.22.input_layernorm.weight', 'phi_model.base_model.model.model.layers.22.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.23.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.23.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.23.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.23.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.23.input_layernorm.weight', 'phi_model.base_model.model.model.layers.23.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.24.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.24.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.24.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.24.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.24.input_layernorm.weight', 'phi_model.base_model.model.model.layers.24.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.25.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.25.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.25.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.25.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.25.input_layernorm.weight', 'phi_model.base_model.model.model.layers.25.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.26.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.26.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.26.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.26.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.26.input_layernorm.weight', 'phi_model.base_model.model.model.layers.26.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.27.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.27.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.27.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.27.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.27.input_layernorm.weight', 'phi_model.base_model.model.model.layers.27.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.28.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.28.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.28.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.28.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.28.input_layernorm.weight', 'phi_model.base_model.model.model.layers.28.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.29.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.29.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.29.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.29.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.29.input_layernorm.weight', 'phi_model.base_model.model.model.layers.29.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.30.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.30.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.30.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.30.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.30.input_layernorm.weight', 'phi_model.base_model.model.model.layers.30.post_attention_layernorm.weight', 'phi_model.base_model.model.model.layers.31.self_attn.o_proj.weight', 'phi_model.base_model.model.model.layers.31.self_attn.qkv_proj.weight', 'phi_model.base_model.model.model.layers.31.mlp.gate_up_proj.weight', 'phi_model.base_model.model.model.layers.31.mlp.down_proj.weight', 'phi_model.base_model.model.model.layers.31.input_layernorm.weight', 'phi_model.base_model.model.model.layers.31.post_attention_layernorm.weight', 'phi_model.base_model.model.model.norm.weight', 'phi_model.base_model.model.lm_head.weight'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load(\"your_model_path/videochat2/videochat2_phi3_stage3.pth\", \"cpu\")\n",
    "\n",
    "\n",
    "if 'model' in state_dict.keys():\n",
    "    msg = model.load_state_dict(state_dict['model'], strict=False)\n",
    "else:\n",
    "    msg = model.load_state_dict(state_dict, strict=False)\n",
    "print(msg)\n",
    "\n",
    "model = model.to(torch.device(cfg.device))\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(conv):\n",
    "    ret = conv.system + conv.sep\n",
    "    for role, message in conv.messages:\n",
    "        if message:\n",
    "            ret += role + message + conv.sep\n",
    "        else:\n",
    "            ret += role\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_prompt2(conv):\n",
    "    ret = conv.system + conv.sep\n",
    "    count = 0\n",
    "    for role, message in conv.messages:\n",
    "        count += 1\n",
    "        if count == len(conv.messages):\n",
    "            ret += role + message\n",
    "        else:\n",
    "            if message:\n",
    "                ret += role + message + conv.sep\n",
    "            else:\n",
    "                ret += role\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_context_emb(conv, model, img_list, answer_prompt=None, print_res=False):\n",
    "    if answer_prompt:\n",
    "        prompt = get_prompt2(conv)\n",
    "    else:\n",
    "        prompt = get_prompt(conv)\n",
    "    if print_res:\n",
    "        print(prompt)\n",
    "    if '<VideoHere>' in prompt:\n",
    "        prompt_segs = prompt.split('<VideoHere>')\n",
    "    else:\n",
    "        prompt_segs = prompt.split('<ImageHere>')\n",
    "    assert len(prompt_segs) == len(img_list) + 1, \"Unmatched numbers of image placeholders and images.\"\n",
    "    with torch.no_grad():\n",
    "        seg_tokens = [\n",
    "            model.phi_tokenizer(\n",
    "                seg, return_tensors=\"pt\", add_special_tokens=i == 0).to(\"cuda:0\").input_ids\n",
    "            # only add bos to the first seg\n",
    "            for i, seg in enumerate(prompt_segs)\n",
    "        ]\n",
    "        seg_embs = [model.phi_model.base_model.model.model.embed_tokens(seg_t) for seg_t in seg_tokens]\n",
    "#         seg_embs = [model.phi_model.model.embed_tokens(seg_t) for seg_t in seg_tokens]\n",
    "    mixed_embs = [emb for pair in zip(seg_embs[:-1], img_list) for emb in pair] + [seg_embs[-1]]\n",
    "    mixed_embs = torch.cat(mixed_embs, dim=1)\n",
    "    return mixed_embs\n",
    "\n",
    "\n",
    "def ask(text, conv):\n",
    "    conv.messages.append([conv.roles[0], text])\n",
    "        \n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    def __init__(self, stops=[], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = stops\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for stop in self.stops:\n",
    "            if torch.all((stop == input_ids[0][-len(stop):])).item():\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def answer(conv, model, img_list, do_sample=True, max_new_tokens=200, num_beams=1, min_length=1, top_p=0.9,\n",
    "               repetition_penalty=1.0, length_penalty=1, temperature=1.0, answer_prompt=None, print_res=False):\n",
    "    stop_words_ids = [\n",
    "        torch.tensor([32000]).to(\"cuda:0\"),\n",
    "        torch.tensor([32007]).to(\"cuda:0\")] \n",
    "    stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n",
    "    \n",
    "    conv.messages.append([conv.roles[1], answer_prompt])\n",
    "    embs = get_context_emb(conv, model, img_list, answer_prompt=answer_prompt, print_res=print_res)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.phi_model.generate(\n",
    "            inputs_embeds=embs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            num_beams=num_beams,\n",
    "            do_sample=do_sample,\n",
    "            min_length=min_length,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            length_penalty=length_penalty,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "    output_token = outputs[0]\n",
    "    if output_token[0] == 0:  # the model might output a unknow token <unk> at the beginning. remove it\n",
    "            output_token = output_token[1:]\n",
    "    if output_token[0] == 1:  # some users find that there is a start token <s> at the beginning. remove it\n",
    "            output_token = output_token[1:]\n",
    "    output_text = model.phi_tokenizer.decode(output_token, add_special_tokens=False)\n",
    "    output_text = output_text.split('<|end|>')[0] \n",
    "#     output_text = output_text.split('<|end|>')[-1].strip()\n",
    "    conv.messages[-1][1] = output_text + '<|end|>'\n",
    "    return output_text, output_token.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(num_frames, num_segments):\n",
    "    seg_size = float(num_frames - 1) / num_segments\n",
    "    start = int(seg_size / 2)\n",
    "    offsets = np.array([\n",
    "        start + int(np.round(seg_size * idx)) for idx in range(num_segments)\n",
    "    ])\n",
    "    return offsets\n",
    "\n",
    "\n",
    "def load_video(video_path, num_segments=8, return_msg=False, resolution=224, norm=\"\"):\n",
    "    if \"s3://\" in video_path:\n",
    "        video_bytes = client.get(video_path)\n",
    "        vr = VideoReader(io.BytesIO(video_bytes), ctx=cpu(0), num_threads=1)\n",
    "    else:\n",
    "        vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "    num_frames = len(vr)\n",
    "    frame_indices = get_index(num_frames, num_segments)\n",
    "\n",
    "    # transform\n",
    "    crop_size = resolution\n",
    "    scale_size = resolution\n",
    "    if norm == \"vit\":\n",
    "        input_mean = [0.485, 0.456, 0.406]\n",
    "        input_std = [0.229, 0.224, 0.225]\n",
    "    else:\n",
    "        input_mean = [0.48145466, 0.4578275, 0.40821073]\n",
    "        input_std = [0.26862954, 0.26130258, 0.27577711]\n",
    "\n",
    "    transform = T.Compose([\n",
    "        GroupScale(int(scale_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        GroupCenterCrop(crop_size),\n",
    "        Stack(),\n",
    "        ToTorchFormatTensor(),\n",
    "        GroupNormalize(input_mean, input_std) \n",
    "    ])\n",
    "\n",
    "    images_group = list()\n",
    "    for frame_index in frame_indices:\n",
    "        img = Image.fromarray(vr[frame_index].numpy())\n",
    "        images_group.append(img)\n",
    "    torch_imgs = transform(images_group)\n",
    "    if return_msg:\n",
    "        fps = float(vr.get_avg_fps())\n",
    "        sec = \", \".join([str(round(f / fps, 1)) for f in frame_indices])\n",
    "        # \" \" should be added in the start and end\n",
    "        msg = f\"The video contains {len(frame_indices)} frames sampled at {sec} seconds.\"\n",
    "        return torch_imgs, msg\n",
    "    else:\n",
    "        return torch_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sinusoid_encoding_table(n_position=784, d_hid=1024, cur_frame=8, ckpt_num_frame=4, pre_n_position=784): \n",
    "    ''' Sinusoid position encoding table ''' \n",
    "    # TODO: make it with torch instead of numpy \n",
    "    def get_position_angle_vec(position): \n",
    "        return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)] \n",
    "    \n",
    "    # generate checkpoint position embedding\n",
    "    sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(pre_n_position)]) \n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) # dim 2i \n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2]) # dim 2i+1 \n",
    "    sinusoid_table = torch.tensor(sinusoid_table, dtype=torch.float, requires_grad=False).unsqueeze(0)\n",
    "    \n",
    "    print(f\"n_position: {n_position}\")\n",
    "    print(f\"pre_n_position: {pre_n_position}\")\n",
    "    \n",
    "    if n_position != pre_n_position:\n",
    "        T = ckpt_num_frame # checkpoint frame\n",
    "        P = 14 # checkpoint size\n",
    "        C = d_hid\n",
    "        new_P = int((n_position // cur_frame) ** 0.5) # testing size\n",
    "        if new_P != 14:\n",
    "            print(f'Pretraining uses 14x14, but current version is {new_P}x{new_P}')\n",
    "            print(f'Interpolate the position embedding')\n",
    "            sinusoid_table = sinusoid_table.reshape(-1, T, P, P, C)\n",
    "            sinusoid_table = sinusoid_table.reshape(-1, P, P, C).permute(0, 3, 1, 2)\n",
    "            sinusoid_table = torch.nn.functional.interpolate(\n",
    "                sinusoid_table, size=(new_P, new_P), mode='bicubic', align_corners=False)\n",
    "            # BT, C, H, W -> BT, H, W, C ->  B, T, H, W, C\n",
    "            sinusoid_table = sinusoid_table.permute(0, 2, 3, 1).reshape(-1, T, new_P, new_P, C)\n",
    "            sinusoid_table = sinusoid_table.flatten(1, 3)  # B, THW, C\n",
    "    \n",
    "    if cur_frame != ckpt_num_frame:\n",
    "        print(f'Pretraining uses 4 frames, but current frame is {cur_frame}')\n",
    "        print(f'Interpolate the position embedding')\n",
    "        T = ckpt_num_frame # checkpoint frame\n",
    "        new_T = cur_frame # testing frame\n",
    "        # interpolate\n",
    "        P = int((n_position // cur_frame) ** 0.5) # testing size\n",
    "        C = d_hid\n",
    "        sinusoid_table = sinusoid_table.reshape(-1, T, P, P, C)\n",
    "        sinusoid_table = sinusoid_table.permute(0, 2, 3, 4, 1).reshape(-1, C, T)  # BHW, C, T\n",
    "        sinusoid_table = torch.nn.functional.interpolate(sinusoid_table, size=new_T, mode='linear')\n",
    "        sinusoid_table = sinusoid_table.reshape(1, P, P, C, new_T).permute(0, 4, 1, 2, 3) # B, T, H, W, C\n",
    "        sinusoid_table = sinusoid_table.flatten(1, 3)  # B, THW, C\n",
    "        \n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_position: 3136\n",
      "pre_n_position: 784\n",
      "Pretraining uses 4 frames, but current frame is 16\n",
      "Interpolate the position embedding\n",
      "The video contains 16 frames sampled at 0.3, 0.9, 1.5, 2.2, 2.8, 3.4, 4.0, 4.7, 5.3, 5.9, 6.5, 7.2, 7.8, 8.4, 9.0, 9.6 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls><source src=\"./demo/example/yoga.mp4\" type=\"video/mp4\"></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_path = \"./demo/example/yoga.mp4\"\n",
    "# vid_path = \"./demo/example/jesse_dance.mp4\"\n",
    "\n",
    "\n",
    "# num_frame = 8\n",
    "num_frame = 16\n",
    "# resolution = 384\n",
    "resolution = 224\n",
    "# vid, msg = load_video(vid_path, num_segments=num_frame, return_msg=True, resolution=resolution)\n",
    "vid, msg = load_video(vid_path, num_segments=num_frame, return_msg=True, resolution=resolution, norm=\"vit\")\n",
    "new_pos_emb = get_sinusoid_encoding_table(n_position=(resolution//16)**2*num_frame, cur_frame=num_frame)\n",
    "model.vision_encoder.encoder.pos_embed = new_pos_emb\n",
    "\n",
    "print(msg)\n",
    "    \n",
    "# The model expects inputs of shape: T x C x H x W\n",
    "TC, H, W = vid.shape\n",
    "video = vid.reshape(1, TC//3, 3, H, W).to(\"cuda:0\")\n",
    "\n",
    "img_list = []\n",
    "with torch.no_grad():\n",
    "    image_emb, _ = model.encode_img(video, \"Watch the video and answer the question.\")\n",
    "#     image_emb, _ = model.encode_img(video, \"\")\n",
    "\n",
    "img_list.append(image_emb)\n",
    "\n",
    "HTML(f'<video alt=\"test\" controls><source src=\"{vid_path}\" type=\"video/mp4\"></video>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "<Video><VideoHere></Video> The video contains 16 frames sampled at 0.3, 0.9, 1.5, 2.2, 2.8, 3.4, 4.0, 4.7, 5.3, 5.9, 6.5, 7.2, 7.8, 8.4, 9.0, 9.6 seconds.\n",
      "Describe the video in detail.<|end|>\n",
      "<|assistant|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/petrelfs/likunchang/.conda/envs/mistral/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video shows a woman performing a yoga pose on a mat. She is wearing a black tank top and black leggings. The woman is standing on her hands and legs, with her hands on the ground and her legs extended upwards. She is wearing a black and white tank top and black leggings. The video captures her in a yoga pose, with her hands and legs in a stretched position.\n"
     ]
    }
   ],
   "source": [
    "chat = EasyDict({\n",
    "    \"system\": \"\",\n",
    "    \"roles\": (\"<|user|>\\n\", \"<|end|>\\n<|assistant|>\"),\n",
    "    \"messages\": [],\n",
    "    \"sep\": \"\"\n",
    "})\n",
    "\n",
    "# chat.messages.append([chat.roles[0], \"\"])\n",
    "# ask(\"<Video><VideoHere></Video> Describe the video in details.\", chat)\n",
    "ask(f\"<Video><VideoHere></Video> {msg}\\nDescribe the video in detail.\", chat)\n",
    "\n",
    "llm_message = answer(conv=chat, model=model, do_sample=False, img_list=img_list, max_new_tokens=128, print_res=True)[0]\n",
    "print(llm_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "<Video><VideoHere></Video> The video contains 16 frames sampled at 0.3, 0.9, 1.5, 2.2, 2.8, 3.4, 4.0, 4.7, 5.3, 5.9, 6.5, 7.2, 7.8, 8.4, 9.0, 9.6 seconds.\n",
      "Describe the video in detail.<|end|>\n",
      "<|assistant|>\n",
      "The video shows a woman performing a yoga pose on a mat. She is wearing a black tank top and black leggings. The woman is standing on her hands and legs, with her hands on the ground and her legs extended upwards. She is holding her arms up in the air and maintaining a balanced position. The video captures her graceful movements and the serene atmosphere of the yoga practice.\n"
     ]
    }
   ],
   "source": [
    "chat = EasyDict({\n",
    "    \"system\": \"\",\n",
    "    \"roles\": (\"<|user|>\\n\", \"<|end|>\\n<|assistant|>\"),\n",
    "    \"messages\": [],\n",
    "    \"sep\": \"\"\n",
    "})\n",
    "\n",
    "# chat.messages.append([chat.roles[0], \"\"])\n",
    "# ask(\"<Video><VideoHere></Video> Describe the video in details.\", chat)\n",
    "ask(f\"<Video><VideoHere></Video> {msg}\\nDescribe the video in detail.\", chat)\n",
    "\n",
    "llm_message = answer(conv=chat, model=model, do_sample=False, img_list=img_list, max_new_tokens=128, print_res=True)[0]\n",
    "print(llm_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"./demo/example/dog.png\"\n",
    "# img_path = \"./demo/example/bear.jpg\"\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "plt.imshow(img)\n",
    "\n",
    "resolution = 224\n",
    "# resolution = 384\n",
    "new_pos_emb = get_sinusoid_encoding_table(n_position=(resolution//16)**2, cur_frame=1, ckpt_num_frame=1, pre_n_position=14*14)\n",
    "model.vision_encoder.encoder.img_pos_embed = new_pos_emb\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            (resolution, resolution), interpolation=InterpolationMode.BICUBIC\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "img = transform(img).unsqueeze(0).unsqueeze(0).cuda()\n",
    "img_list = []\n",
    "with torch.no_grad():\n",
    "#     image_emb, _ = model.encode_img(img, \"\")\n",
    "    image_emb, _ = model.encode_img(img, \"Observe the image and answer the question.\")\n",
    "img_list.append(image_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = EasyDict({\n",
    "    \"system\": \"\",\n",
    "    \"roles\": (\"<|user|>\\n\", \"<|end|>\\n<|assistant|>\"),\n",
    "    \"messages\": [],\n",
    "    \"sep\": \"\"\n",
    "})\n",
    "\n",
    "ask(f\"<Image><ImageHere></Image> Describe the following image in detail.\", chat)\n",
    "\n",
    "llm_message = answer(conv=chat, model=model, do_sample=False, img_list=img_list, max_new_tokens=128, print_res=True)[0]\n",
    "print(llm_message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral",
   "language": "python",
   "name": "mistral"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
